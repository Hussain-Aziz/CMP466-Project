{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import value_counts\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv as data frame \n",
    "df = pd.read_csv(\"coupon.csv\")\n",
    "\n",
    "# drop cars as it is mostly blank (108 are blank, 12576 are empty)\n",
    "df = df.drop(columns=['car'])\n",
    "\n",
    "# drop direction_opp as it is inverse of direction_same so its redundant\n",
    "df = df.drop(columns=['direction_opp'])\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# list of columns with blank values\n",
    "blank_columns = ['Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']\n",
    "\n",
    "# delete 42 common null values\n",
    "df.dropna(subset=blank_columns, how='all', inplace=True)\n",
    "\n",
    "# null impute based on average probalistic weightage in each of the classes\n",
    "for column in blank_columns:\n",
    "    weights = df[column].value_counts(normalize=True)\n",
    "    df[column].fillna(pd.Series(np.random.choice(weights.index, size=len(df.index), p=weights.values.tolist())), inplace=True)\n",
    "\n",
    "df.isnull().sum()\n",
    "# Ordinal Encoding\n",
    "\n",
    "# a common classes order for all some columns\n",
    "amount_visited_order = ['never','less1','1~3','4~8','gt8']\n",
    "\n",
    "# ordinal columns with their categories in order\n",
    "ordinal_columns = [('temperature',['30','55','80']), \n",
    "                   ('time', ['7AM', '10AM', '2PM', '6PM', '10PM']), \n",
    "                   ('expiration', ['2h', '1d']),\n",
    "                   ('gender', ['Male', 'Female']), # since its 2 values we can do ordinal encoding\n",
    "                   ('age', ['below21','21','26','31','36','41','46','50plus']), \n",
    "                   ('education', ['Some High School', 'High School Graduate', 'Some college - no degree', 'Associates degree', 'Bachelors degree', 'Graduate degree (Masters or Doctorate)']),\n",
    "                   ('income', ['Less than $12500','$12500 - $24999','$25000 - $37499','$37500 - $49999','$50000 - $62499','$62500 - $74999','$75000 - $87499','$87500 - $99999','$100000 or More']),\n",
    "                   ('Bar', amount_visited_order),\n",
    "                   ('CoffeeHouse', amount_visited_order),\n",
    "                   ('CarryAway', amount_visited_order),\n",
    "                   ('RestaurantLessThan20', amount_visited_order),\n",
    "                   ('Restaurant20To50', amount_visited_order)]\n",
    "\n",
    "# apply the ordinal encoding\n",
    "for column, categories in ordinal_columns:\n",
    "    df[column] =  OrdinalEncoder(categories=[categories], dtype=np.uint8).fit_transform(df[[column]])\n",
    "\n",
    "# to check if it worked\n",
    "# df.to_csv(os.path.join('PreprocessingData',\"coupon_processed_2.csv\"), index=False)\n",
    "# Nominal Encoding\n",
    "\n",
    "nomimal_columns = ['destination', 'passanger', 'weather', 'coupon', 'maritalStatus', 'occupation']\n",
    "\n",
    "for column in nomimal_columns:\n",
    "    df = pd.get_dummies(df, columns=[column]) # type: ignore\n",
    "\n",
    "# to check if it worked\n",
    "# df.to_csv(os.path.join('PreprocessingData',\"coupon_processed_2.csv\"), index=False)\n",
    "\n",
    "# for machine learning, we need arrays so we extract y as array\n",
    "y = df[\"Y\"].to_numpy()\n",
    "\n",
    "# drop the target column\n",
    "df = df.drop(columns=[\"Y\"]) \n",
    "\n",
    "# extract x as array\n",
    "x = df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training (80%) and testing (20%)\n",
    "x_train, x_test, y_train, y_test, = train_test_split(x,y, test_size=0.2)\n",
    "\n",
    "# Create classifier decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create decision tree classifer object\n",
    "classifier = DecisionTreeClassifier(ccp_alpha=0.001)\n",
    "\n",
    "# train decision tree classifer\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict the response for test dataset\n",
    "y_predicted = classifier.predict(x_test)\n",
    "\n",
    "# check accuracy\n",
    "print(f\"training accuracy {classifier.score(x_train, y_train)}\")\n",
    "print(f\"testing accuracy {classifier.score(x_test, y_test)}\")\n",
    "\n",
    "print(f\"node count {classifier.tree_.node_count}\")\n",
    "print(f\"depth {classifier.get_depth()}\")\n",
    "print(f\"number of leaves {classifier.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune\n",
    "\n",
    "def tune(start, stop, step, classifier, x, y, tuning_range=None):\n",
    "    data = []\n",
    "    current_percent = 10\n",
    "    start_time = time.time()\n",
    "    if tuning_range is None:\n",
    "        tuning_range = range(start, stop, step) if type(start) == int else np.arange(start, stop, step)\n",
    "    for i in tuning_range:\n",
    "        node_count = []\n",
    "        testing_accuracy = []\n",
    "        training_accuracy = []\n",
    "        for _ in range(50):\n",
    "            \n",
    "            # create decision tree classifer object\n",
    "            classifier_tune = classifier(i)\n",
    "\n",
    "            # split data\n",
    "            x_train, x_test, y_train, y_test, = train_test_split(x,y, test_size=0.2)\n",
    "\n",
    "            # train decision tree classifer\n",
    "            classifier_tune.fit(x_train, y_train)\n",
    "\n",
    "            # get traning and testing accuracy\n",
    "            node_count.append(classifier_tune.tree_.node_count)\n",
    "            training_accuracy.append(classifier_tune.score(x_train, y_train))\n",
    "            testing_accuracy.append(classifier_tune.score(x_test, y_test))\n",
    "\n",
    "        \n",
    "        # get average of 50 runs\n",
    "        node_count = sum(node_count) / len(node_count)\n",
    "        training_accuracy = sum(training_accuracy) / len(training_accuracy)\n",
    "        testing_accuracy = sum(testing_accuracy) / len(testing_accuracy)\n",
    "\n",
    "        data.append([i, node_count, training_accuracy, testing_accuracy])\n",
    "\n",
    "        if step != 0:\n",
    "            if (i - start) // step > ((stop - start) // step + 1) * current_percent/100:\n",
    "                print(f\"{current_percent}% done at {(time.time()-start_time)/60} minutes\")\n",
    "                current_percent += 10\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot(data, title):\n",
    "    # convert data to data frame\n",
    "    df = pd.DataFrame(data, columns=[\"param\", \"node_count\", \"training_accuracy\", \"testing_accuracy\"])\n",
    "\n",
    "    # plot node count vs training accuracy and testing accuracy and label the graph\n",
    "    df.drop(columns=['param']).plot(x=\"node_count\", y=[\"training_accuracy\", \"testing_accuracy\"])\n",
    "    plt.xlabel(\"node count\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def save_to_csv(model, name):\n",
    "    df = pd.DataFrame(model, columns=[name, \"node_count\", \"training_accuracy\", \"testing_accuracy\"])\n",
    "    \n",
    "    if not os.path.exists(\"TuningData\"):\n",
    "        os.makedirs(\"TuningData\")\n",
    "        \n",
    "    df.to_csv(os.path.join(\"TuningData\", name + \".csv\"), index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min sample split\n",
    "min_sample_split = tune(2, 2000, 5, lambda i: DecisionTreeClassifier(min_samples_split=i), x, y)\n",
    "plot(min_sample_split, \"min sample split\")\n",
    "\n",
    "min_sample_split_df = save_to_csv(min_sample_split, \"min_sample_split\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion{“gini”, “entropy”, “log_loss”}\n",
    "\n",
    "criterion = tune(0, 0, 0, lambda i: DecisionTreeClassifier(criterion=i), x, y, [\"gini\", \"entropy\", \"log_loss\"])\n",
    "\n",
    "criterion_df = save_to_csv(criterion, \"criterion\")\n",
    "\n",
    "criterion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitter{“best”, “random”}\n",
    "\n",
    "splitter = tune(0, 0, 0, lambda i: DecisionTreeClassifier(splitter=i), x, y, [\"best\", \"random\", \"random\"])\n",
    "\n",
    "splitter_df = save_to_csv(splitter, \"splitter\")\n",
    "\n",
    "splitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth\n",
    "# with default params: 41\n",
    "max_depth = tune(1, 60, 2, lambda i: DecisionTreeClassifier(max_depth=i), x, y)\n",
    "plot(max_depth, \"max depth\")\n",
    "\n",
    "max_depth_df = save_to_csv(max_depth, \"max_depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_samples_leaf \n",
    "# The minimum number of samples required to split an internal node:\n",
    "min_samples_leaf = tune(2, 5000, 10, lambda i: DecisionTreeClassifier(min_samples_leaf=i), x, y)\n",
    "plot(min_samples_leaf, \"min samples leaf\")\n",
    "\n",
    "min_samples_leaf_df = save_to_csv(min_samples_leaf, \"min_samples_leaf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_weight_fraction_leaf\n",
    "# The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. \n",
    "# Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "min_weight_fraction_leaf = tune(0.0, 0.1, 0.0001, lambda i: DecisionTreeClassifier(min_weight_fraction_leaf=i), x, y)\n",
    "plot(min_weight_fraction_leaf, \"min_weight_fraction_leaf\")\n",
    "\n",
    "min_weight_fraction_leaf_df = save_to_csv(min_weight_fraction_leaf, \"min_weight_fraction_leaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features\n",
    "# If int, then consider max_features features at each split.\n",
    "max_features = tune(1, x.shape[1]+1, 1, lambda i: DecisionTreeClassifier(max_features=i), x, y)\n",
    "plot(max_features, \"max_features\")\n",
    "\n",
    "max_features_df = save_to_csv(max_features, \"max_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_leaf_nodes\n",
    "# with default params: 2716\n",
    "\n",
    "max_leaf_nodes = tune(2, 3000, 10, lambda i: DecisionTreeClassifier(max_leaf_nodes=i), x, y)\n",
    "plot(max_leaf_nodes, \"max_leaf_nodes\")\n",
    "\n",
    "max_leaf_nodes_df = save_to_csv(max_leaf_nodes, \"max_leaf_nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_impurity_decrease\n",
    "\n",
    "min_impurity_decrease = tune(0.0, 0.005, 0.00001, lambda i: DecisionTreeClassifier(criterion='entropy', min_impurity_decrease=i), x, y)\n",
    "plot(min_impurity_decrease, \"min_impurity_decrease\")\n",
    "\n",
    "min_impurity_decrease_df = save_to_csv(min_impurity_decrease, \"min_impurity_decrease (entropy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccp_alpha\n",
    "\n",
    "ccp_alpha = tune(0.0, 0.01, 0.00001, lambda i: DecisionTreeClassifier(ccp_alpha=i), x, y)\n",
    "plot(ccp_alpha, \"ccp_alpha\")\n",
    "\n",
    "ccp_alpha_df = save_to_csv(ccp_alpha, \"ccp_alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max = 0\n",
    "\n",
    "# Create a decision tree classifier object\n",
    "dtc_tuned = DecisionTreeClassifier(\n",
    "                                criterion='entropy',\n",
    "                                splitter='best', \n",
    "                                max_depth=60, \n",
    "                                min_samples_split=160, \n",
    "                                min_samples_leaf=80, \n",
    "                                min_weight_fraction_leaf=0.007,\n",
    "                                max_features=60, \n",
    "                                max_leaf_nodes=190, \n",
    "                                min_impurity_decrease=0.00065,\n",
    "                                ccp_alpha=0.00031\n",
    "                        )\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(dtc_tuned, x, y, cv=10)\n",
    "\n",
    "print(np.mean(cv_results['test_score']) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
